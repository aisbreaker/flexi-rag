"""
Definition of the RAG-workflow
for generating answers to questions in a chat.

Here we use a single node workflow graph,
which is a simple way to define a workflow with unified workflow state (AnswerWorkflowGraphState).
All the complex/hierarchical RAG logic and data structures are encapsulated in this node.
"""

from typing import Dict, List, Optional
import langchain_core
from typing_extensions import TypedDict

from langchain.schema import Document
from langchain import hub
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import AnyMessage
from langchain_core.messages.system import SystemMessage
from langgraph.graph import END, StateGraph, START
from factory.llm_factory import get_default_chat_llm_with_streaming, get_default_chat_llm_without_streaming
from rag_workflow.chat_workflow_tools import Question, enrich_questions_with_retrieved_documents
from rag_workflow.chat_workflow_tools import default_llm_with_streaming, default_llm_without_streaming

import logging
                           
logger = logging.getLogger(__name__)


#
# Type(s)
#

class AnswerWorkflowGraphState(TypedDict):
    """
    Represents the state of our LangGraph graph.
    """

    # All messages, i.e. the full context of the chat conversation, including the question
    messages: List[AnyMessage]

    # The answer generated by the LLM
    generation: Optional[str]

    # Config
    stream_generate_on_last_node: Optional[bool] = False


#
# LangGraph node(s)
#
async def generate_chat_answer_node(
        state: AnswerWorkflowGraphState,
        config: RunnableConfig
    ) -> Dict:
    """
    Generate answer

    Args:
        state (dict): The current graph state
        config (RunnableConfig): The current runnable configuration
             Note on Python < 3.11
             https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/
                "When using python 3.8, 3.9, or 3.10, please ensure you manually pass the RunnableConfig through to the llm when invoking it like so: llm.ainvoke(..., config)."

    Returns:
        state (dict): New key added to state: generation, that contains LLM generation
    """

    messages = state["messages"]
    streaming = state["stream_generate_on_last_node"]

    # Prompt: Don't pull because it's not working in air-gapped mode
    #   or if api.hub.langchain.com:443 is down as of 2024-09-12
    #prompt = hub.pull("rlm/rag-prompt")
    #   prompt: input_variables=['context', 'question'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:"))]
    #systemPrompt = "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:"
    systemPromptStr = "You are an assistant for question-answering tasks. Use the provided documents or pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise."

    # Add prompt as system message at the top to all the messages
    messages = [SystemMessage(content=systemPromptStr)] + messages

    # LLM
    if not streaming:
        llm = get_default_chat_llm_without_streaming()
    else:
        llm = get_default_chat_llm_with_streaming()

    # This is where we're adding a tag that we'll be using later
    # to filter the outputs of the final node for streaming-mode
    llm = llm.with_config(tags=["final_node"])

    # Add context to the question(s)
    messages = await enrich_questions_with_retrieved_documents(messages, config)

    # Run
    if not streaming:
        logger.info("llm invoke (not streaming) ...")
        # Note on Python < 3.11
        #   https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/
        #   "When using python 3.8, 3.9, or 3.10, please ensure you manually pass the RunnableConfig through to the llm when invoking it like so: llm.ainvoke(..., config)."
        generation = await llm.ainvoke(messages, config=config)
        logger.info(f"llm await done, generation: '{generation}'")
    else:
        logger.info("llm invoke (streaming/async) ...")
        generation = await llm.ainvoke(messages, config=config)
        logger.info(f"llm await done, generation: '{generation}'")

    return {"generation": generation}


#
# LangGraph workflow - chat inclusive RAG
#

def create_workflow():
    """
    Create the workflow.
    
    Keep in mind that all the (complex) RAG logic is in a single node.
    """
    workflow = StateGraph(AnswerWorkflowGraphState)

    # Define the node(s) - they contain the actual RAG logic
    workflow.add_node("generate", generate_chat_answer_node)

    # Build the graph
    workflow.add_edge(START, "generate")
    workflow.add_edge("generate", END)
 
    # Compile
    app = workflow.compile()
    return app
