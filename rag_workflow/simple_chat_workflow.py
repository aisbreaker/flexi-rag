"""
Defines a simple workflow that uses a LLM to generate an answer. No RAG.
"""

from typing import Dict, List, Optional
from typing_extensions import TypedDict
from langgraph.graph import END, StateGraph, START

from langchain_core.runnables import RunnableConfig
from langchain_core.messages import AnyMessage

import logging
                           
from factory.llm_factory import get_default_llm_with_streaming, get_default_llm_without_streaming

logger = logging.getLogger(__name__)



#
# Type(s)
#

class AnswerWorkflowGraphState(TypedDict):
    """
    Represents the state of our LangGraph graph.
    """

    # All messages, i.e. the full context of the chat conversation, including the question
    messages: List[AnyMessage]

    # The answer generated by the LLM
    generation: Optional[str]

    # Config
    stream_generate_on_last_node: Optional[bool] = False


#
# LangGraph node(s)
#

async def generate_chat_answer_node(
        state: AnswerWorkflowGraphState,
        config: RunnableConfig
    ) -> Dict:

    """
    Generate answer

    Args:
        state (dict): The current graph state
        config (RunnableConfig): The current runnable configuration
             Note on Python < 3.11
             https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/
                "When using python 3.8, 3.9, or 3.10, please ensure you manually pass the RunnableConfig through to the llm when invoking it like so: llm.ainvoke(..., config)."

    Returns:
        state (dict): New key added to state: generation, that contains LLM generation
    """

    messages = state["messages"]
    streaming = state["stream_generate_on_last_node"]

    # LLM
    if not streaming:
        llm = get_default_llm_without_streaming()
    else:
        llm = get_default_llm_with_streaming()

    # This is where we're adding a tag that we'll be using later
    # to filter the outputs of the final node for streaming-mode
    llm = llm.with_config(tags=["final_node"])

    # Run
    if not streaming:
        logger.info("llm invoke (not streaming) ...")
        # Note on Python < 3.11
        # https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/
        #   "When using python 3.8, 3.9, or 3.10, please ensure you manually pass the RunnableConfig through to the llm when invoking it like so: llm.ainvoke(..., config)."
        generation = await llm.ainvoke(messages, config=config)
        logger.info(f"llm await done, generation: '{generation}'")
    else:
        logger.info("llm invoke (streaming/async) ...")
        generation = await llm.ainvoke(messages, config=config)
        logger.info(f"llm await done, generation: '{generation}'")

    return {"generation": generation}


#
# LangGraph workflow - simple LLM chat without implementing a RAG
#

def create_workflow():
    workflow = StateGraph(AnswerWorkflowGraphState)

    # Define the node(s)
    workflow.add_node("generate", generate_chat_answer_node)

    # Build the graph
    workflow.add_edge(START, "generate")
    workflow.add_edge("generate", END)
 
    # Compile
    app = workflow.compile()
    return app
