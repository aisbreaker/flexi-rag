#
# configure the Retrieval Augmented Generation system here
# (in this file are the default settings/default values)
#
config:
  common:
    llms:
      ChatOpenAI_default_llm:
        module: langchain_openai
        class: ChatOpenAI
        args:
          # you can overwrite, e.g. "model_name" with environment variable:
          #   export RAG_CONFIG__COMMON__DEFAULT_LLM_WITH_STREAMING__ChatOpenAI__args__model_name="gpt-ais-invalid-model"
          model_name: "gpt-4o"
          temperature: 0.2
          streaming: False

      ChatOpenAI_default_llm_with_streaming:
        module: langchain_openai
        class: ChatOpenAI
        args:
          model_name: "gpt-4o"
          temperature: 0.2
          streaming: True

      ChatOpenAI_strict_llm:
        module: langchain_openai
        class: ChatOpenAI
        args:
          model_name: "gpt-4o"
          temperature: 0
          streaming: False

  rag_index_service:
    enabled: true
  crawling:
    enabled: true
    depth: 3
    max_pages: 1000


  rag_response_service:
    default_llm: ChatOpenAI_default_llm
    default_llm_with_streaming: ChatOpenAI_default_llm_with_streaming
    document_grader_llm: ChatOpenAI_strict_llm
    rewrite_question_llm: ChatOpenAI_strict_llm


  embedding_and_indexing:
    enabled: true
    embedding_model: "bert-base-uncased"
    index_path: "/data/aisbreaker-workspace/hapkecom-github/flexi-rag/index"

  vector_search:
    enabled: true
    index_path: "/data/aisbreaker-workspace/hapkecom-github/flexi-rag/index"
    similarity_threshold: 0.7

  string_keyword_search:
    enabled: true
    index_path: "/data/aisbreaker-workspace/hapkecom-github/flexi-rag/index"

  final_llm_processing:
    enabled: true
    model: "gpt2"
    max_length: 100
    temperature: 0.8


# test entries without any semantics:
test:
  value: 1
  valuestr: "test"
